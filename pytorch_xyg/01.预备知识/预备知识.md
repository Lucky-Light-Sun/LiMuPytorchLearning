# 大致介绍

主要针对数据的存储、操作和预处理。更详细内容参考笔记。

## ndarray简介

```python
import torch

# 创建
torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
x = torch.arange(12)
torch.zeros((2, 3, 4))
torch.ones((2, 3, 4))
torch.randn(3, 4)   # Norm(0, 1)


# 基本属性和操作
len(x)  # 第一位形状
x.shape
x.numel()
x.dim()
X = x.reshape(3, 4) # 注意，X, x 对应的元素共用存储空间 id(X[0, 0]) == id(x[0])
x.reshape(-1,4)

# 类型转换
A = X.numpy()
B = torch.tensor(A)
type(A), type(B)    # (numpy.ndarray, torch.Tensor)

a = torch.tensor([3.5])
a, a.item(), float(a), int(a)


# 广播机制
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2)) # 甚至 b = torch.arange(2).reshape((2)) 也是可行的, 但是 a reshape(3) 是不靠谱的
print((a + b).shape)

# 索引和切片
X[-1], X[1:3]
X[1, 2] = 9
X[0:2, :] = 12          # 区域赋值还是蛮有意思的

# 节省内存
before = id(Y)
Y = Y + X   # 错误示范
id(Y) == before # False

X[:] = X + Y    # 正确操作
X += Y          # 正确操作

```

## pandas简单

```python
import os
import pandas as pd
import torch

# 创建并读取 csv 文件
os.makedirs(os.path.join('..', 'data'), exist_ok=True)
data_file = os.path.join('..', 'data', 'house_tiny.csv')
with open(data_file, 'w') as f:
    f.write('NumRooms,Alley,Price\n')  # 列名
    f.write('NA,Pave,127500\n')  # 每行表示一个数据样本
    f.write('2,NA,106000\n')
    f.write('4,NA,178100\n')
    f.write('NA,NA,140000\n')

data = pd.read_csv(data_file)
print(data)

# 缺失值处理 (NaN 常见处理方式为插值法和删除法)
inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]
inputs = inputs.fillna(inputs.mean())   # 数值 NaN 用平均
print(inputs)

inputs = pd.get_dummies(inputs, dummy_na=True)  # 其他的类似于类别 one-hot
print(inputs)

# 转换为张量形式
X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)
X, y
```

## 线性代数

本小结仅介绍线代在pytorch中的简单使用，概念方面参考手写笔记

```python
# 基本运算
A = torch.arange(20 * 2, dtype=torch.float32).reshape(5, 4, 2)
print(A.T)  # 转置
B = A.clone()  # 通过分配新内存，将A的一个副本分配给B
A.sum(axis=[0, 2], keepdim=True)
A.mean(axis=0) == A.sum(axis=0) / A.shape[0]
A.cumsum(axis=0)

# 乘法运算
x, y = torch.tensor([2, 3]), torch.tensor([2, 1])
torch.dot(x, y)   # 必须为一维输入
(x * y).sum()     # 等价做法

mat = torch.randn(2, 3)
vec = torch.randn(3)
torch.mv(mat, vec)      # tensor([ 1.0404, -0.6361])

x = torch.eye(3, 4)
y = torch.ones(4, 5)
torch.mm(x, y)

input = torch.randn(10, 3, 4)
mat2 = torch.randn(10, 4, 5)
res = torch.bmm(input, mat2)    # torch.Size([10, 3, 5])

# 范数 -> 距离的衡量
u = torch.tensor([3.0, -4.0])
torch.norm(u)   # 二范数 - tensor(5.)
torch.abs(u).sum()  # 一范数 - tensor(7.)

torch.norm(torch.ones((4, 9)))  # 矩阵的 Frobenius 范数


# 手动求梯度
x = torch.arange(4.0)
x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)
y = 2 * torch.dot(x, x)
y.backward()
x.grad == 4 * x
x.grad.zero_()
u = y.detach()
# 我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和
```
