# linear

## linear-scratch

```python
import torch
import random

def data_iter(batch_size, features, labels):
    num_samples = len(features)
    indices = torch.arange(num_samples)
    random.shuffle(indices)
    for i in range(0, num_samples, batch_size): # 注意这个step的应用，不然很麻烦
        j = min(num_samples, i + batch_size)
        batch_indices = indices[i:j]
        yield features[batch_indices], labels[batch_indices]


def sgd(params, lr):
    with torch.no_grad():   # 一定要注意这个操作
        for param in params:
            param -= param.grad * lr    # .grad
            param.grad.zero_()  # .grad.zero_()


w = torch.normal(0, 1, (2, 3), requires_grad=True)
b = torch.zeros(3)
b.requires_grad_(True)
```

## linear-concise

```python
def data_iter(data_arrays, batch_size, is_train=True):
    dataset = TensorDataset(*data_arrays)
    return DataLoader(dataset, batch_size, shuffle=is_train)

def main():
    """
        ...
        ...
    """
    net = nn.Sequential(
        nn.Linear(in_features, out_features)
    )
    net[0].weight.data.normal_(0, 0.01)
    net[0].bias.data.fill_(0)

    lr = 0.01
    num_epoches = 10
    optimizer = torch.optim.SGD(net.parameters(), lr)
    loss = torch.nn.MSELoss()

    for epoch in range(num_epoches):
        for X, y in dataloader:
            y_hat = net(X)
            l = loss(y_hat, y)
            optimizer.zero_grad()
            l.backward()
            optimizer.step()
        l = loss(net(features), labels)
        print(f'epoch {epoch}, loss {l}.')
```

## linear 小结

**学习率**
LR过大模型震荡大，出现容易NaN
LR过小效率低，学习缓慢

**batch size**
过大，内存吃不住，且如果部分样本相同会浪费算力
过小，不适合并行最大利用计算机资源。
batch size小的话会引入噪声，但是噪声对网络而言不一定是坏事，能够提高模型的鲁棒性、泛化性。

## softmax

在平板上的笔记更为详细

***一些简单的 trick***

1. 损失函数使用 Huber's Robust Loss 的 tricks 方案
2. 因 softmax 特性难以逼近 one-hot encoding，进而使用 softlabel 方法

```python
# pytorch 自带的 dataset
torchvision.datasets.FashionMNIST

def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):
    # 注意 scale 是先 cols, 然后才是 rows
    figsize = (num_cols * scale, num_rows * scale)
    fig, axes = plt.subplots(num_rows, num_cols, figsize=figsize)
    axes = axes.flatten()

    for i, (ax, img) in enumerate(zip(axes, imgs)):
        if torch.is_tensor(img):
            ax.imshow(img.numpy())
        else:
            ax.imshow(img)
        ax.axes.get_xaxis().set_visible(False)
        ax.axes.get_yaxis().set_visible(False)
        if titles:
            ax.set_title(titles[i])
    plt.show()

# 网络的初始化方法
net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights)
```





