# 权重衰退和丢弃法(weight decay & dropout)

当网络模型过拟合时候，我们可以从模型复杂度角度和数据角度进行处理。
数据角度包括收集更多的数据，并进行适当的数据增强操作
模型角度包括降低网络模型的参数量，又会是通过weight decay 和 dropout 方法。本节对两种方法简要介绍。

## 权重衰退(weight decay)

1. 本质是限制模型参数的范围，空间范围缩小
2. 权重衰退lambda取值常为 0.01, 0.001, 0.0001(weight decay)

分为硬性限制和柔性限制，现主要使用柔性限制。

```python
# scratch
def l2_penalty(w):
    return torch.sum(w.pow(2)) / 2
l = loss(net(X), y) + lambd * l2_penalty(w)


# concise
net = nn.Sequential(nn.Linear(num_inputs, 1))
for param in net.parameters():
    param.data.normal_()
loss = nn.MSELoss(reduction='none')
num_epochs, lr = 100, 0.003
# 偏置参数没有衰减
trainer = torch.optim.SGD([
    {"params":net[0].weight,'weight_decay': wd},
    {"params":net[0].bias}], lr=lr)

params_list = []
for module in modules_ori:  # resnet 部分
    params_list.append(dict(params=module.parameters(), lr=args.base_lr))
for module in modules_new:
    if module is not None:
        params_list.append(dict(params=module.parameters(), lr=args.base_lr * 10))
args.index_split = 5
optimizer = torch.optim.SGD(params_list, lr=args.base_lr, momentum=args.momentum, weight_decay=args.weight_decay)
```

## 丢弃法

1. 好的模型需要对输入数据的扰动鲁棒
   1. 具有噪声的数据等价于 Tikhonov 正则
   2. Dropout 方法在层之间加入噪声
2. Dropout 实现时候保证了 期望 E 不变。而且只是在训练中使用，推理过程中 h=dropout(h) 以保证模型的确切输出
   1. 如今普遍认为 Dropout 是正则化
   2. 起先认为是随机采样子神经网络训练，而后AVG
3. Dropout p 取值往往为 **0.1, 0.5, 0.9**
4. Dropout就是正则化，不合理的选择会引起 under/over -fitting
5. 大模型 高 Dropout 有时比 小模型低 Dropout 好一些(**Test**)
6. 控制模型大小方法
   1. CNN 使用 BatchNorm 正则化
   2. MLP 使用 Dropout 丢弃法
   3. 通用方法 Weight Decay，但是难调，效果也不好说


```python
# torch.rand 是[0, 1)之间的 uniform 随机数
# 对 GPU 而言，Mask 乘法更快
def dropout_layer(X, p):
    assert 0 <= p <= 1
    if p == 0:
        return X
    elif p == 1:
        return torch.zeros_like(X)
    mask = (torch.rand(X.shape) >= p).float()  # [0, 1)
    return mask * X / (1 - p)

#  模型初始化 Gauss 分散鼓励模型将权重分配到各种特征中，而不依赖于少数的虚假关联，这个 Dropout 出发点类似
def init_weights(layer):    # net.apply(init_weights)
    if isinstance(layer, nn.Linear):
        nn.init.normal_(layer.weight.data)
        nn.init.normal_(layer.bias.data)
```
