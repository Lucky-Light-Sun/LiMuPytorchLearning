# 单层感知机

## MLP的注意事项和实现

1. 激活函数常常使用ReLU，因为其它激活函数作用类似，但是其速度更快
2. MLP的参数设计网络追求更深，而不是更宽(而且常常会在第一层expand特征维度一下)
3. MLP设计过程中一定要记得激活函数的加入，否则会退化为一个线性层
4. SVM和MLP区别
   - 如今MLP更常用，因为他更易替换为 CNN、RNN结构，而SVM替换起来没那么方便
   - 原来因为 MLP 调参过多，且可解释性差，故对参数不敏感、可解释性更强的SVM更受欢迎

```python
# 和 requires_grad 那一套做法类似
w1 = nn.Parameter(
    torch.normal(0, 1, (in_features, mid_features))
)

def concise():
    def init_weight(m):
        if isinstance(m, torch.nn.Linear):
            nn.init.normal_(m.weight, 0, 0.01)
        
    net = nn.Sequential(
        nn.Flatten(),   # flatten() 操作
        
        nn.Linear(in_features, mid_features),
        nn.ReLU(),
        nn.Linear(mid_features, out_features)
    )
    net.apply(init_weight)  # apply
    ...

    plt.ioff()
    plt.show()
```


