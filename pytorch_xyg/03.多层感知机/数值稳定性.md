# 数值稳定性

## 数值稳定性介绍

1. 根据神经网络和反向传播的特性，容易引起梯度爆炸和梯度消失的问题，常发生于深度学习中，因为矩阵的累乘
2. 梯度爆炸
   1. 数值溢出，尤其对于 float16
   2. 学习率不易调整(太大容易发散且恶性循环，太小训练不动，甚至需要在训练过程中动态调整)
   3. 网络每个层都训练不好，不同梯度消失是因为他前几层一直在变化，后几层梯度合适也跟不上前几层的剧烈变化
3. 梯度消失
   1. 梯度值太小，精度保留到0
   2. 因为梯度为0，导致训练不动
   3. 网络模型可能后面几层训练较好，导致很深的模型退化为很浅模型

## 解决方法

让训练过程更加稳定的方法

1. 乘法变为加法
   - ResNet、LSTM
2. 归一化方法
   - 梯度归一化(u=0, std=1)，梯度裁剪(阈值clip)
3. *合理的权值初始化和激活函数*

下文主要介绍合理的权值初始化和激活函数
**权值初始化** Xavier 使得$\gamma_t = 2 / (n_{t-1} + n_{t})$，即

- 正态分布 $N(0, \sqrt{2 / (n_{t-1} + n_{t})})$
- 均匀分布 $U(-\sqrt{6 / (n_{t-1} + n_{t})}, \sqrt{6 / (n_{t-1} + n_{t})})$

**激活函数**因为要向 f(x) = x靠拢，常用形势为

- tanh(x)
- relu(x)
- 4 * sigmoid(x) - 2

## 经典 QA

1. INF和NaN基本都是梯度爆炸导致，其中NaN常常是除0。缓解方法可以通过
   1. 使用合理的权值初始化(没什么想法就用 Xavier)
   2. 激活函数可使用 relu(), tanh(), 4 * sigmoid(x) - 2
   3. 学习率调整，往往是一直往下降，直到现象明显缓解在逐渐往上升(加快训练速度)
2. 需要注意模型本身的数值稳定性
   1. 比如ResNet和LSTM本身就是一个很好的数值稳定模型
3. 将某一层 mean=0, std=1并不会损失其表达能力，从数学上来看，数据所包含的信息量未变，只是调整到模型更以训练的数值范围。
