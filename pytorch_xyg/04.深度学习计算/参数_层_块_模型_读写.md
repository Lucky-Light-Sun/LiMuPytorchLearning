# 参数、层、块、模型、读写

神经元->层->块->模型，是设计模式在深度学习项目中的应用

## 模型构造

1. super.__init__() 作用
2. self._modules: OrderDict
3. self.test_weight 可以在 net.state_dict(), net.paramters()中获取，但是 self._modules 中不会
4. 倘若自己定义了 self.block_list = [], net._modules(), net.state_dict(), net.parameters() 可能获取不到
5. nn.Sequential() 使用不方便的只能自己定义网络了

```python
"""
nn.Sequential`定义了一种特殊的`Module
它维护了一个由`Module`组成的有序列表。
"""
nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))


"""
super().__init__() 执行必要的初始化
Initializes internal Module state
    self.training = True <--------
    self._parameters: Dict[str, Optional[Parameter]] = OrderedDict() <--------
    self._buffers: Dict[str, Optional[Tensor]] = OrderedDict()
    self._non_persistent_buffers_set: Set[str] = set()
    self._backward_hooks: Dict[int, Callable] = OrderedDict()
    self._is_full_backward_hook = None
    self._forward_hooks: Dict[int, Callable] = OrderedDict()
    self._forward_pre_hooks: Dict[int, Callable] = OrderedDict()
    self._state_dict_hooks: Dict[int, Callable] = OrderedDict()
    self._load_state_dict_pre_hooks: Dict[int, Callable] = OrderedDict()
    self._modules: Dict[str, Optional['Module']] = OrderedDict()    <--------
"""
class MLP(nn.Module):
    def __init__(self) -> None:
        super().__init__() 
        ......

    ......

# 使用 self._modules 好处多多
# 在块的参数初始化时，系统知道在_module字典中查找需要初始化的参数子块
#     如果 self.test_weight = nn.Parameter(torch.ones(3, 3))
#     那么他不会出现在 self._module 有序字典中
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            # 变量_modules中。_module的类型是OrderedDict
            self._modules[str(idx)] = module # 等价于 self.add_module(f'{idx}', module)

    def forward(self, X):
        # OrderedDict保证了按照成员添加的顺序遍历它们
        for block in self._modules.values():
            X = block(X)
        return X

# 测试使用 list 存储 block，不适用 _module
# state_dict() 拿不到这部分参数，但是可以拿到 self.test_weight 参数， net.parameters()和他一样
# self._module 哪个层都拿不到
class TestSequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        self.test_module_list = []
        self.test_weight = nn.Parameter(torch.ones(3, 3))
        for idx, module in enumerate(args):
            self.test_module_list.append(module)

    def forward(self, X):
        for block in self.test_module_list:
            X = block(X)
        return X

net = TestSequential(nn.Linear(10, 3), nn.ReLU(), nn.Linear(3, 2))
X = torch.ones((3, 10))
Y = net(X)
print(net.test_module_list[2].weight.grad)  # None
Y.sum().backward()
print(net.test_module_list[2].weight.grad)  
        # tensor([[0.0000, 0.7996, 0.0000], [0.0000, 0.7996, 0.0000]])
print(net.state_dict())
        # OrderedDict([('test_weight', tensor([[1., 1., 1.],
        # [1., 1., 1.],
        # [1., 1., 1.]]))])
print(net._modules)
        # OrderedDict()
print('-' * 20)
print(net.parameters()) # 取之于 self.named_parameters(recurse=recurse)
for param in net.parameters():
    print(param)
        # --------------------
        # <generator object Module.parameters at 0x000001EE5A8A04A0>
        # Parameter containing:
        # tensor([[1., 1., 1.],
        #         [1., 1., 1.],
        #         [1., 1., 1.]], requires_grad=True)
```

## 参数管理

1. nn.Paramter()
2. 定义非可学习参数 net.parameters(), net.state_dict()
3. layer.bias, layer.weight, layer.weight.data, layer.weight.grid
4. print(*[(name, param.shape) for name, param in net.named_parameters()])  # net.parameters()就是调用的这个
5. net.state_dict()['2.bias'].data
6. net.add_module(f'block {i}', block1()) # 就是 self._modules[name] = module
7. def init_normal(m): nn.init.normal_(m.weight, mean=0, std=0.01), net.apply(init_normal)
8. m.weight.data = torch.tensor(3.8)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F


class TestModule(nn.Module):
    def __init__(self):
        super().__init__()
        self.weight_have_grad1 = nn.Parameter(torch.ones(1))    # net.parameters(), net.state_dict() 可以获取
        self.weight_have_grad2 = torch.ones(2, requires_grad=True)  # 上述两种均获取不了
        self.weight_have_not_grad = torch.ones(2, requires_grad=False)  # 上述两种均获取不了
    
    def forward(self):
        pass

module = TestModule()
print('-' * 20)
print(module.parameters())
for block in module.parameters():
    print(block)
print('\n', '+' * 20, sep='')
print(module.state_dict())

--------------------
<generator object Module.parameters at 0x000001F9ACCF8430>
Parameter containing:
tensor([1.], requires_grad=True)

++++++++++++++++++++
OrderedDict([('weight_have_grad1', tensor([1.]))])


print(type(net[2].bias))
print(net[2].bias)
print(net[2].bias.data)


# 初始化
def init_xavier(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)   # nn.init.xavier_uniform_(m.weight.data) 
net[0].apply(init_xavier)

# 你也可以自己手动整活
m.weight.data *= m.weight.data.abs() >= 5   # 注意这个灵魂 .data，要不然整体赋值之后，id都变了

# 共享参数
shared = nn.Linear(8, 8)
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                    shared, nn.ReLU(),
                    shared, nn.ReLU(),
                    nn.Linear(8, 1))

```

## 自定义层

1. self.bias = nn.Parameter(torch.randn(units,))  # 注意使用使用了 nn.Parameter(), 别仅仅使用 requires_grid=True 了

```python
class MyLinear(nn.Module):
    def __init__(self, in_units, units):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, units))
        self.bias = nn.Parameter(torch.randn(units,))
    def forward(self, X):
        linear = torch.matmul(X, self.weight.data) + self.bias.data
        return F.relu(linear)
```

## 读写文件

1. torch.save()
2. torch.load()
3. net.state_dict()
4. net.load_state_dict()

```python
# torch.save(), torch.load()
mydict = {'x': x, 'y': y}
torch.save(mydict, 'mydict')
mydict2 = torch.load('mydict')
mydict2


# 模型参数的保存和加载
net = MLP()
torch.save(net.state_dict(), 'mlp.params')

clone = MLP()
clone.load_state_dict(torch.load('mlp.params'))
clone.eval()
```
